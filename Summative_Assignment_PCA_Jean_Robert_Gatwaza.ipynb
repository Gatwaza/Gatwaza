{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gatwaza/Gatwaza/blob/main/Summative_Assignment_PCA_Jean_Robert_Gatwaza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Motivating PCA**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\\\n",
        "The goal of PCA is to extract information while reducing the number of features\n",
        "from a dataset by identifying which existing features relate to another. The crux of the algorithm is trying to determine the relationship between existing features, called principal components, and then quantifying how relevant these principal components are. The principal components are used to transform the high dimensional data to a lower dimensional data while preserving as much information. For a principal component to be relevant, it needs to capture information about the features. We can determine the relationships between features using covariance."
      ],
      "metadata": {
        "id": "xyATLU4z1cYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "UTntK0eUNimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([\n",
        "    [   1,   2,  -1,   4,  10],\n",
        "    [   3,  -3,  -3,  12, -15],\n",
        "    [   2,   1,  -2,   4,   5],\n",
        "    [   5,   1,  -5,  10,   5],\n",
        "    [   2,   3,  -3,   5,  12],\n",
        "    [   4,   0,  -3,  16,   2],\n",
        "])\n",
        "\n",
        "# Calculate the mean of each feature\n",
        "mean_vector = np.mean(data, axis=0)\n",
        "\n",
        "# Center the data by subtracting the mean\n",
        "centered_data = data - mean_vector\n",
        "\n",
        "# Calculate the covariance matrix\n",
        "covariance_matrix = np.cov(centered_data, rowvar=False)\n",
        "\n",
        "print(\"Covariance Matrix:\")\n",
        "print(covariance_matrix)"
      ],
      "metadata": {
        "id": "qWaiAdz8PyKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b02991a-db34-451e-f154-0d75c6ecc3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance Matrix:\n",
            "[[  2.16666667  -1.06666667  -1.76666667   5.5         -4.36666667]\n",
            " [ -1.06666667   4.26666667   0.46666667  -6.6         19.66666667]\n",
            " [ -1.76666667   0.46666667   1.76666667  -3.3          2.36666667]\n",
            " [  5.5         -6.6         -3.3         24.7        -27.9       ]\n",
            " [ -4.36666667  19.66666667   2.36666667 -27.9         92.56666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QeWiytCjPhKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Standardize the Data along the Features\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> ![image.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Explain why we need to handlethe data on the same scale.\n",
        "\n",
        "**[TO DO: Insert Answer here]**"
      ],
      "metadata": {
        "id": "U2U2_Q5ebos3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#standardized_data = #TO DO:  Insert code here\n",
        "# Given dataset\n",
        "data = np.array([\n",
        "    [1, 2, -1, 4, 10],\n",
        "    [3, -3, -3, 12, -15],\n",
        "    [2, 1, -2, 4, 5],\n",
        "    [5, 1, -5, 10, 5],\n",
        "    [2, 3, -3, 5, 12],\n",
        "    [4, 0, -3, 16, 2],\n",
        "])\n",
        "\n",
        "# Calculate the mean and standard deviation of each feature\n",
        "mean_vector = np.mean(data, axis=0)\n",
        "std_dev_vector = np.std(data, axis=0)\n",
        "\n",
        "# Standardize the data\n",
        "standardized_data = (data - mean_vector) / std_dev_vector\n",
        "\n",
        "# Print the standardized data\n",
        "print(\"Standardized Data:\")\n",
        "print(standardized_data)\n",
        "\n",
        "\n",
        "#Why Standardize the Data?\n",
        "\n",
        "#----> Scale Independence:\n",
        "\n",
        "#Standardizing the data ensures that all features are on the same scale. This is important in PCA because it is a variance-based method, and the variance of each feature contributes to the principal components. Features on different scales may have different variances, and this could lead PCA to give more weight to features with larger variances.\n",
        "\n",
        "#----> Equal Weight to Features:\n",
        "\n",
        "#PCA aims to find the directions (principal components) of maximum variance. If one feature has a larger scale than another, it might dominate the variance computation and, consequently, dominate the principal components. Standardizing the features ensures that all features contribute equally to the variance calculation.\n",
        "\n",
        "#----> Numerical Stability:\n",
        "\n",
        "#Standardization improves the numerical stability and convergence of certain algorithms. It can help prevent issues related to scale when performing computations, especially in optimization algorithms.\n",
        "\n",
        "#----> Interpretability:\n",
        "\n",
        "#Standardized data makes the interpretation of principal components more straightforward. The coefficients in the principal component vectors represent the importance of each standardized feature in the corresponding principal component.\n",
        "\n",
        "\n",
        "#By standardizing the data, we make sure that the PCA algorithm focuses on the underlying patterns and relationships in the data without being influenced by differences in the scales of the original features. It helps ensure that the principal components capture the true structure of the data in terms of relative variances across features."
      ],
      "metadata": {
        "id": "JF3eGB7FRC0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74523b68-1ac3-4f61-e1d3-dd5227bcd3b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized Data:\n",
            "[[-1.36438208  0.70710678  1.5109662  -0.99186978  0.77802924]\n",
            " [ 0.12403473 -1.94454365 -0.13736056  0.77145428 -2.06841919]\n",
            " [-0.62017367  0.1767767   0.68680282 -0.99186978  0.20873955]\n",
            " [ 1.61245155  0.1767767  -1.78568733  0.33062326  0.20873955]\n",
            " [-0.62017367  1.23743687 -0.13736056 -0.77145428  1.00574511]\n",
            " [ 0.86824314 -0.35355339 -0.13736056  1.65311631 -0.13283426]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![cov matrix.webp](data:image/webp;base64,UklGRno5AABXRUJQVlA4WAoAAAAYAAAAzwIAmAAAQUxQSJw3AAAB/yckSPD/eGtEpO7hj///1VKy7Xt9Pt+99iQM3d3dPQxdw9DdDCCNNAgWDQZKgyAGYWArdndwGCgqdrcedtf3j7X2Wmvv2TPocV3XfUf0fwL4J1GovPOiNIRCcc2ezihF21V7uqMU0ksuGYLwj/DJqr8MRQuBCDvmXvgkpihjuHDx+ucxhZJh7tYpX6L/ACmtfqc9heRfDcqVpIj7W/PSZSicDR8Pj1Tln+Gl9wAqhYH+iVulKPMX4YoWAvCH8M9w+qxvn5tdmkJQGHCPXZ5NUVbIu8OuyEFCKASFblfbVf2Qf4Lgp2xI63lm0kO4/GpIGXfmxYIUTRD23IQSZpkV7ZDkBiz/DwYFlX94hKgF2ix8Akl2yhszoIdh0TGKqsorCzCgoP5KdX54NJrkDHdtx/BPsNL9R4CSd1EI2jpkPp8KNhMpGCohqISgWnDANibki4YlPfimP5h2ZHXO+ofHsPkhHKh3f9JTqlpgUgb8RhFVKG+B1ufn9Jy+DNR4g+HS4clOwGZC59VHWzd87h8eh+cXY6B28nOY+A4KsH47WkD6t0AC9WqDBBC6d0QKiGHUB1C7yVkn4Ad8K3uTnqHVz1Cpypw74Mvi/+zA3w1RKQzgxv04OLTaT4G1T2MCCPYVTKBfP8IUELj6EBTj6Q5UeQfyzl6waNGihSur4BQCsO4xFE60o847/MNb8hdAaHA/kvQ+6oJCzXMouOlK8DRD8JQIBfe9Xhj41HDuFVC3Wwd31ywM+0clvydmYOA9w4XnU/qfG8OdW0bsxyBk3kxSV7r9knESBDpB45IFJbkrnf5Mew2UXs/BO1U6puP7gh5JTahmU18RDINfh7db9Un550ZZvn59Kgg11v00oGQyE5odWtEDhQc/euKtpymoEoZICCIFQ2h4+Iw+qGHTBrhtbDM0phDt8OYNjUhq5a5dMgZR8kbBoLkl+Ue3Cp4VmlRrnpLMgPQyuGvVrdOoQoFJ+inlcKcBVMV/k5r1aiczQCrzb6y4irjiKqzFJQIi/0CoJJLEEJFkJ4Jb3EUVEXxKABGRZCcUUJVTo/+xlvgJo7sgQUQlXiLhSAEQlXiJhCOFiajESyQciYckkEjcJBzxI4HOKI0U+eKvNPkrEkQEQMKQWEKogl9JEBEAiYsQXykERAAkLkLyNgASknoIia7MeoOif22JF/ydi8G3QJmOjQh5onhQqmwYpFWLMZkEVaiUXRckmJA+3ouKWeHU90qZlPQEKmTXAwkjZYIX5UuEUz8O4zORBIHmHTIIt1dLBHf9cOr6GFsC8QX85wJMEW/FkluLxUe58kkCCpHDF+ePnVJpTRiL6iKgdN1wRX80gFBz68WnoyDUX5kglLz+vIkTxtY8JxicWR4BZdiqa9shwbYuucaD9qcnOaH4kfMnjZ9Q7dwwVlRGQBm0+upsNNjFS46EJZRZT6JOv2HBiHn1J3dGAggNF+C9bdnhMNYvusVLKL6BgAK2KlqEE2p9kGaLIXFQStlM1JeQ+sMAgPveQAMonZahuL+odCI/kOGFHoe3YwDlnC5o/JTKv7QDOPEQGkAZPB3F/W3mp73RABFm3NT6v15sboAkMaHsL50Ann8SDaDkzkZxf1viw7xAhsl3NP0uLJQZY9CE2PsYQLZtjAaAXQ4CERZf3enzQEqXF8r+5YVh0mTUFxEueouivLLxHuL+0j0YAr69CmMcGj8RCC6tjIBS0xJcwBJTqLoHiR98P5mIRul1BwZQH3BVOoCS8x2hPjsXEEBpfT6axODrmUQ0Spd7MYD6gIPFAZR2PxHqU8uIp3OIBFR6WIgYeLATDqB+ss/D4H51KsGFa/bi+yoCCtg+mKKb4dgqUImDUs0WR30pDSze64CUKBiIAIbMW/Be8BqCBoCuPwPqAo5GiLvS2+LprAZSIxABBSLUuAoBhAsfBTSI8nM2bgG4kySudLV46mog1QEDBjBUPoIAwvqnAA2i/NAzHBGXcHUNJAE+XEMUDGO7AplgIOJStnVBAeH3NmCCGN6f4sdwsD7iiwg7/6TILlQ8y+6dRlyVu0+g+HZYHMuBuYOHLyfzzp8bw8mn6HglBpCx3zw7pzKBc1/9dF5TvA07ctB4OWz/BHXhwIL+YxdS4YkvqpD66VGGX4gBUqf9+cDsUgQUmuyz546ABqmAcrABkrQcLvwv6sKB+XmjFlH2ka9r4nx4FwO2YYDo5N8fnlOWgEKjS+zqEUgI3oYVk9C4ge2KwS10mtNzZXNu+a0VvHAsjXvLIgjNrrBnDyWgUHOT3TzJ16LpqD8B2x4torn/AJpdtAkNScD2x/gzrPs+Blw+FsYegC+A6SnMXeoB7+ZA9al3+lNuXwLlhz5QAsFw9iJMvAwHTsaAm3rBwoup+BEwB9ZMRHF/UwPqnH6DL5TsdyA6csjh2WDYNRpNWoZL34kB1+XC3B2U+gSYDWedhuL+sh7UnnuzL5TWH4BLgjS7eBOKYewOTLwEbHvUg4b3Ak9E+QqYmg734VZ6vwpOp5uGorEQSv4IqKAKKMP3YvwhvHoCKaoZRn+Loez8vZiQDGdaghqm21hlfgH4vha3LqNqHqw/zSVgS0HFNs/4g7+7Qfmaz2R6LF0VP4dN38Rq+jXAr2k8OYoG3WBvnksoa4EKPR7yZzhwOwwoDr/WJMrFMzBJy2HVj4hXg28AfsnikXzq94RdQ11CllWo2PVRf4Z99xElRCm7YC8GZciV8QNsX4wryr2LyOTcO7huNRUHQeZjHobrjmDSSx0a78uw7A2ixFb6XRPIMNRSZDfsvBUDuXvRkOD7OzEBhOI2grpyOv0Jwo+9afE+gyrChmkupdGfIPCES4yHIBYEHvVaviZ+Sj0LAqS1HvklArYh/Y8zNhP25bqUfl+BUOJBl2is12bCo2PhiRmksn1mPFRDUA1BNTSlhgUFou0GfYOAbU6vVxlVEnYPcSldvwMh/SGXaKyX5gPVoK4JwIBLUJThB3HiZnjsIFFQuuiHo3CY/jb1PyW3GhTzUt4dj4GLPVRi3LoNmFy95PARHnlHAgnY+ZgimvLGPIwyfF9YQpodGQjlprtBlWb91AL8XRFe7tyVCAuWugwrjmMg4ylXTKXXj7if8jp7SfyAZ/aDUXLaV/0TwCq806QHEdZNcBn23IGBqg+5/Np6UDodPmlCCjvHo+Elw0cPglGys8tbAJsCb7boToRVU12GLfdjoOKjLr+2CTRbfkXPNneUQWKCMmovBsOY3Zi4KZVsFAws55HlFGPzrfB87y4AD3qArQqGnR4+vxkAuR1+6sHDI1BlyGWBEN76iCK6gq0ByvC9YRlmWoMEAR7fDlRZYrhyIUy/GoeRthiGnP0YJMp9612pT4My9EaPFM67CwM86bW1Oxo3gRPnAPUWwK1j4ZztpLD0B8FhxCYMRDk53VX5IVCmX+4RoZLFc9YVoBxogoR3+UI00O4VaABhyxo0PI6vAeosgpsnwxl7iDLnZ4PD4IswEOWF013lHwVlygGPCGUtpOQ0+xqOd8evMsJr2VQ0bijD36sHTOpCg2PAC5VggC2Nwp1lEBxq/k0MA/e1QSECNhWt0OVpeGAYapg/K5hhpS2qQW1LvJ75gxAFZu09Z8GkCLBgwsTZIJgrQci6Gc+fO6BC2tNgmGsrIQAvz/MDR9NIQIFle84+fRzA8tFTp4BQZh8ItQ8hALYaIh6GTTYFAZj9Goqh0S7cdxPPdw5gAggnbgjh2N1xEFh8ydkLxgMsGztlGghZ+0GodjUCYOug4mFYbdMRgGnvgOHypfBBOtmHduzatWvHwaE4sa6qj8QPqLFv09J5LRHarRx0Rj0EDgKGbTkosOQl1Ifd74JuP4Lh0nPgM8BwoBkSrKHtXkRryPgHMa7dmHDAHsUEQ4BUg6dx8CnsLQ/p9fkRd/rjuBvUcLXn2ywEeKwYCBUuRxIAAdIFEIhE8H0wCqUrl/0CEKrej7t1WYCOXH0mBrJmQyo0247GIRkKkK54RiL4viIdSlbN+hoQKj6Eu1V5VweuWIOB463pc4xiRNXtCMqI3RjgWhJTQTLwTsenkr0GQzY3L8Rg2D4OBfoDtOSsQzjwShMm3kxT4AjBBezeIpliJ1zZFUUYsJ9whQy7JBSIEFOJKYDSfTFc++SIXRiA5zxmRXBY8EubOxCA/wgoi7qjiQCG4AIoI6fCsavnnI0Bsp7wmA2CsU0fS0UwUw2d28CaJkgcREIQCUEkHhAhuADKoBnw2I3T12GA9Kc8ZuOyzR/JRCn5I9w1uWEtfAoD94MybjyaEIgCAiggLsFzr5JuGz6puHeMQYTOPVGwvW6rh1Lue9h2XquyMCwfDQT88X6RTBi8dCTuXjc8OzktFMNg2xQNJY5n1aLJ4vmA0HjTu2fVgiZdQSi/cGkmIlQ9493NLaD6Sgr8mjK0XzILEFrteGtxZWjXBgGmLuuIKOt/PPnRJ5VpuYDCe00F2i6ZCwgttr69rAq0aY8AU5blIEK7bTBsTQN897rp2cnpZK6lgCut5sNpy9oh4Iw9drQ/MBb32GX9AbK6QGRYaUhdR5jKHbZI5teAIVSHTTaKJFYkH99CiII7P1LQhKwJ+FbCjuRTaAuZE/EtJKQBA+NLIwUL6N+M2BFQwh9THgnBYY1NL5oJ4hG+crMl0UB8+RUQD08hGQogsQKKHwNSWLnVV0DxEJAg3koyjALiEVAQDwUUUEI1jLXNimaJ+PFvFFmFoq9QcCUZJK6EorS1k4pwIhIP+yYajioiSFxEQlGRhFNFBAkjtoiEIiKFhyoiSBixRSQUEYlDAqsgSFxEJAwR8RGuUsyeU4SLp4C9DxOKEMdmKV5xTmmSOPFsqR7xzmxQCMSzpfEogM0TRgldKFHPK/4twxGwe4twNRblhaaUtodDotX685e0JTuY0HkM3rMnpYYxeUY5BGBiFyRBumzctKgxOcGE3IEICNH5YzWMnIUdEIBZzdBk13nD+YuakRNM6D0EAcHMG2fC6LgwOyyh/1AkIRTmX7B6IuVqI0FgWTGvrvPbIUGEJot6eQl9RiIh3V+Ee7a9rYqEVd/uDGnrXTWhw8WPIEFIvRC3cN3wd84lEsCwYdnNd2Bc7EwnMQ8eqUwke/+taACh/Hq87+31Yz6RAEKd+4dYvPeQ7C+/vgqm09470ABC6Q24hbv6fTOTSACh+kMDbFjABTWQBBBqvzkuQs3hb3ZEAwint0dAaHL3KEtQIfWZHJuC4LmxLhLOS0W1CPlvMI+wDe3tmWEYzn4P9zknEVEVMI6AMYaVvVEQopZJJQkq2Kp9GyBAhP7LMfFTLnkG994HUVQFMY6AYwznt0RBqGKZ6RA0ypFtOh4BDPlTiCQxZdvzuLc/jqIqiHEEHGPY0BYFoZxlRipBoxzcxyQkJKXpRYkgYNvjto1RUQXjCBgjlNyPO8rtm1JGIwEcVj/ATLyFutvCAL7/sqgGV68FRCWkHnZOCAK2Cg4CNyIEvQHvTu8CouqPlF8BFM+biL9QzCoGIesqFECIrdfjPfZpQFT9wQetAFEB0q8jiQupNgUHIf0aFEDweT3eQ58HRNUfvJNNuKK4r48Sf8PBh3Agypq+BJ47GYP78waAqPiCx/LxfSSTUD/8u2gmVJj8+7WDHAAJJ89OC8Ew1uLdAarmDqhJ5RWrmuGcvZG6N2AAGfjUh6OrEbjXdX9Nboq3crgmEi+HM39GcLeD2rkDK1L37HNqk7nxXLrsxwApQ187MbwcAYUmS+2aPsS+uXgSc1j8J4K7PdTKHViFWmeeU5+MdatpfwADRIa+fHJEBQIKjRfbDX3D8TRs64rGDewQDAi1qkKPfr0ovmJVK8yZm0pwSx0EodkZ9txeBBRqzLS7B/gwXNgHDeMtWzQD6tvmZak5d3VNJASHwXZ8KKt/jAG9Lk5J392PuRY4upiRmzzgo7PSSZ2wOtcfDx2JpJM+PgoY1k9C42W48g3UA0atjZQ40JYNFnh0AvMWehD5aVpxik9Z08UPMP6X4mVpv2RFFAx7+6BJy7D3PdQDhm1wsi7vyCoLPDiFmWd4IN/PLkax/DXd/QCjfi9ZLpzWvREMp63CxEvAtkPx3t+OvEsZY4EblsO9Bs9p32eUoePSMwwSA+hoq5VDBVWP8RsxYZwsqil1fofMrtT8rAISzohQVvzh48MMMB/As02gL5w9O8YfNaFnOs/MQmMJb42EJlPfz0IwnHEWJl4O+96LZT4BKr0Jr5Um2gO2DUFBwEahl5HXhqCxHOb9B2q1ZthbEGHbFEzSctj+qY9PgDJvw6sVifSEi0ajIGCLQa8IL49EYxmmvwiIiGiAPocOYFCG748fYLMxXmsuAa6aw3/aQi6kPoLbYfnDULclY0/i09Dtc/wqeYfCEF4sqjmseBFWToFdh3DCGRKCkmNjVG77Owg/dOf0+8hpB5umuoTSFtpdBo1+QES8wFZF4L4MjyVr4mcYb/E0ZYZ8gYKty/kHyG0A+/u5lOYW+m+Bvu+hSKyHN8OVzeH10aRy0cx4iIQgEoJIaIbhFk8tm/s1gG3Mumvp0wT2DHYp9S302gHdPkFFvBzu20GEUHvvRFGGHMRJgF/m4rhq8OY4Upj5Oqc9RodOUPwxD8Mzq+FQA3h/EKpeDrtvB8ZVKjZ0oEe/I+E8X1Qz3L8ZBs+GrVeENTQE4M/hpKBkDq1tAX5rB/9lkMA5s13K8M+h4kXQ8EsEb6W2xf1UpseZZ8cPsG1JwVC5Z7cfPCqT9gWjULYPdhmWvAT1N0LvN4jg88fusLQHvDiCVHaeFo9kaLNJwVC+T8dfEbA1cb5kJHDxaJdhzmtQ+zzo+g4Gn9/mQsaixvXHTwdRb0EZtQ+DYdQVCWCY8w2oGPJ5axLpLHwRviIvAmmPeMCv7WBlNrzen9jKK/OgX+fvevPwUFQZdHU4x4tq8F0PFOCd2kjCKM3/Kg0woy5fVIOKnwM7b6pDhBHnuRwuO0IE4OoV4FT2MMw6iQJPeq2diMZN6fsr7gVl+CYKrd5BuOGKakSZuxgDDvddiAG4fyJklEUAQWwK7nLvQ4Q9fdDwKqSHULZYCGVKhKd0/y0CcHoFvsmAph8AVx2sTpTpKzHgcHQ7BuDO0yC9HAII2EwYkP1XLV7thF9lxF6PqaswcQOe3QFQfT4rbwQenA4X3tkAhXsjgJBq8az2DlDbC2w9tGqH5+D+kahh3IZTCUPEKqhywUTCDA2h1WPzWzSf1QWaHapRZXdbHKpZUOpdjyKGdydgiNJ7H7DNCgJRbtuGE0u5uhYSN6Dno9ObtZrbBrrvr1Dr0ro4dPgFlM77MCh83xXFYdyFGG6zuB16fA8YuK4pCrcWJ472P5hA9iQmgPDHJ0TCQuj66KxmLea2hZzLK9fc1xCHFr+D0uEKDCr8tw+Kw6gtCNdb3A7ZP0GqzD4Arzah8vC+ubm5fYfWxcTa3h2Nn8D+Az0a9p8N7B6bOfZ8oIwFlOvqIUTp/xVg4Ma6ELWbiYBDFQuGS1bDpwKGzX3RUwfo9TEgjJtIQiHQY9LgEgAZvfoWw10V9xE8/85ElMZrAZxVdVzw344o8FQW7ltJSIE+k/PSAEr17hPFXRn3tbiNBYQOKwEy15VHALZcjwFWdgLIuIF4tq+NBGpdFwkALRoSR4HekwekA5To3TcFd2Xc1+JpAaHt2QAZaysiAOfdghEeGk7FT6Du6flTpkzJn9cOw5hLMcBNURJRoPqY8S1AoG3vNrireUyfigH2HsIAZ7UFyJuL57TjKLzUnLG30Ri4IYMQi2rKQzmrzsVAVjVojCQQSkwBEJfbsLgPzLm85gsI0BYaAJ3LYuj2hLyH552AkrsMkwgYYgqAutyG9a1g09qO92GAFtAY6J3mejf1wU4o1DNQEybnE4lDUlRiCoC63IZV7WDN+a0fwQCtoDHQM9P1TsY93TDwcRoXros0wKfQ/XxQmlyASQQQws7cB7ynj7VEoQFQA+iPwku1Lp2KQ9mfYNfGFhWgzhbklEF4atEeUFp/uPfA45MwiQRq1AWqeAru6FZYe2hNRxSevWXfXXuh5iwwjH5w0TgU0lr+MTITuDBCoqpRF6KKpwBCuQtg344L66JkvHrN/nsvgCb5AObNqReDMu2d7YefbAmbiatKCCohqMYF1KgLVPEU3KU2w6495zVGSTtxZP+9F0OjqbjfnLoNlPbvwJVndIxgvAUydr5aLgLrKyCJgagRD1HxEJewoCsZb0zdDMrst3YcfqoRzKmOwIlZu0Eo2QdSxpQDVlc7hQByDAh1p0ycNCOTEOMTttJhAjSvBpA+YeLE2U2gVAruWo1xlx4yeFgdGN8BTZSwhd5DoU1ZgJKTJ06cXRvKGRfRHNydpkycMh5mNETikdSFniOgdQWArEkTJ86uA2UdD6cL7qyaQNcIvtsNG9IvSl4eQkE/owRpObg7T5k4ZSxQGc8uIMQW+gxBOJWIt2t4ggENUwAh3tFGJMHGCgjxzqxHYd7YAEIBbEqBFzJrE74AKKACzQi3CKceYowxoQ1OPEAFtzHGiA8VLzVGSJoquI0xRnyIeqgxxlD4q+A2xhjxIeolgAYQY5SkKeqhxhgDiIcoCSk8W1SLdwERElOSgZCYUqgJBVWSRIJKaM+dugxPvH+HheOnLkP/f/+d+gz/XyyJz8iCIoWHFDCREERCEEkSIoWQSIF5qUgXR9fQgvJvr3C8KCflklK5wqNiASvlhJCVGkKxtCSRVrwQckqdehmutknJHsYpDATHrsUUJHsME8i+igkg/PYxJgko7/2JFjKGY/bUS6jRPykNqIYUBsDQMkhB6tEICdSlORIAstsgSUBomU1hKzTqceoV3wL1b28RTzQpqRQaRgqWSgiqIagmCdVCSPRULJ4F6t/e/6Nq8L91z3qIqiSMJpaqaixVlTiI/M/IcxbU4I4kSBJVf8nWNfzfuuMWBVp0bgiiCVElsTq3qVEpVu2qnVr7C5iRnnyG/otHicf/fvLGo7f0/LwlGi/DcHsvmjjOtmetbYKC4TRr71/tK6sFEkNItxbkf0a62Y9SgYx3bVkUNUbAGGNAHUWNUUCMETEOQ+3dCGKMuoyjqFEPNUZdaozEgKY32btcsOM5CyDGKBBl3DOkgTqKMWRYKzjGUdRx5H89otaCMQ7YOji4lcBCqIJvwVtwi5dh6tzLbAoYpve/w2YheArwwGUk8X/51tutpADK3S2B/rOml4fpk0eOhZELxlIzf0ZHoNqU8dQfD+OG5QClx8/sAzgTp9Rn5NzaINBmwdTqADmzpjt4K1NWZNnNROBqjtt0oPyEmT2BygfsU5NmYcZNqRKZWpEeYwbBwqljutFq5jnZyP92PGxnEyFm6vd3dJtm1zHO2oWwzrY7y87o8cOnUP8le9np9ju22A9hiD2/9Ulbisx99rE54zbZQcALPw5dYtfC6x93Xm/bol4Tt/OFBfqv5y3XJHtO+3f/zqD+TvvU9NOJ7rbHp//X0tdaw6zP7ApG2K056P92PGenYmJ9YIGRtgvd7SRYOZwNFsrZFVDW3sn1d4J9BkbZDLA3ANYWA/sk7LdlwV7AzRa44w88lfF7GGFPg8Plec01zQJ2P2DX4LaW/t+lsN8C/PYrN7fFrYy77YqD7gM31kUKxPB/7661ZxDxaEQpey9pVLP3wZ8WXgdoPWm53Qat7Fzc9jmAGtOm2IdcL+JlLQJgf01rFD3fpiNeO8BaGu6Cky6oO2WGvRWq212o6xXcN1lIpbS1Q4i6Crpr4L91x+wgewcGiPCfCjXt7USpbo/DZNth3HoYYu/LGWgvhhZ2EinGq/kfLw5qYx90HXM9hdi/QSDD/jBx6YIlC/BUJu6Elbb2krauTOhgj+V2sjdDJbuLMgr2CRw1Hgj77DpiVs9p3dYzJz000fgMKSgqhYZKwVINQTUE1SShWgipFpjnLa/ZJrjrHwf7NtDAbgXsDw8Yqtm7INOuhkZ2LBHAPoPYLwF7q+tp1yPwsQVowR8Wt8QYcSFgf9sMvGRTSLNvA/YQVLUXckVvsI+iGK51sWjfPDvZQ+i3fe0G97qLKiIhxdM1tKD82ysct/CiXZNF6pjnqsAK2wNetRDlLLsCatknYaW9F3LsHNz2TbA/w3j7MYg96XoJWtrrYd4d9LcXQq93HA+YfQDlClsX5QNbnhT7BUy1r4Ha53m6MtgXcd9rHRj6Njxku7gSUZn6QFJ6KB8tJP7THy1I169EAx1chwYQ9m1Gk4Bw3mVIIaOcdX0BUuj58OvHHtsoCAx+8si9V4IIxV8WoNsrR3c0nPTN9i6v/vz2Fsi8/euv11P/2AO7c3p8fLTW3d9+sbvsI999vg/qP/7wNeuBJo88cO2VxfFMve2zr4/Vo8mtMOPZ9//77CU0ff7e3e37f3w0Srd37pxE6Xu++/CeUrD+8x9vrvmAPQ777HeXe4iJHd5ZnyWlz87EFBK/TMYUpBcvCSQ8fTCEB29MCsqRRwqhS14sQAgxhdhCEb9A/dvrAcYARgCMgBHc6lIFVQHESwBVUFVAXOJSBQVUQSWAqIcAqqAqgAoKSAyJJQkkmpRUCg2VgiUSgkgIIklCpBASKVinngXq397/o2r4v3UvnrqM+J+zof/WHf///fcvzvD/ORv8b92xopxofIYUFJVCQwuYSAgiIYgkCZFCSKTAPF+Ui6draEH5t1c4XpTLaJSUGmUUGs2cglWrdAg1yoVQtWKSKF+tECpd69QrwhGblOxVOIWBELUbcAqSfZZIIPsGkQDC358RSQLKBxYtZCI8a0+9oETLpNSyBIVluzQKdKMKBK9fheB1q5Mcq9Wl8K3Q6FQsngXq394inmhSUik0tICphKASgkiSECmERE/F4lmg/u09pRn2b93zpy4DE0YKjCSeFBgpRCSJSOJJAZIgx4pyIhKXYQFUEEFCiSkiCSIiXqGLIoKEElskQVTEK/mKIoKEEltEEkNFwgtdVBAklNgqkhgiEiugcLwoF0/XUF9CPFt5JX7xBiEJcWye5pX4rZKNEMdmGV7JsKkTjhK60KCkVwGMND+FqLloQKIojLtgw+yStEeCzWiCgFJrUV6CdFvYHgGY3RgNgxbrL1jakZxgSvsJeM/Kz0yMvNObIiBk5ycZmq27YGlnsoMpbSbjPXNK8cTIPb0FEo5QZVk4VF99wYo+tE0JJNSZjXf+jPJI/ITo/HHqIYzuiZwqPNfeVkUSQeDYmmKU6P3oCkwApdcMvJ9rb6sgcROa3jXC4r2HUDffWxvaXfBkMCKbcRuuGvPKJiJxU/pcO+9nF3BWWySZnPdAXWi78Zlg6GbchismvLCdSNyUbtdP/z0slBFT0BDmP9scWi5/jRA3g4Bh47Kb78DED+7r9f1UInhuL8EpQYT815lD2EHg9TW431iKQRXEMSCOgYPiijD1JHNIwCh3bkwZhQCG6RPQIIalH+Fe/DYiqoI6BsRRZckAIiBgGVOG+Ec5PrNCrodQdh/J0zDvC9xzPkBEVcA4AsYYFgwlgtvKyArEP8qx+WUGhAYcJrAy4A/cub+AqAoYR8E4hjGzMIBgq/auj8RNqGSZ7uCpdDsXcyoA164lXNFAylALgkPvczEEbLUVg/vIKsIVDQCfNwBUAIpfS4i2NlGAGwl+A97NPyJkDQA/ZwKigHCoEpIswDYiCnAjwa/Hu8EXhKwB4MeShKsCGM4ejAYAO5AUgBsiBD5UGs/UXwlZA8CYp/F9Mz6LbkKFyX9cO8QJxR3A8Oz1OABZTaFdvwEpdF23tDh19szknGkYhIpT/rhmcCSUgEKzFfbcPsS+NS2IYaDFux1U7j+wLv3Wz0+hxb7R1L0FdQ189PMxNcPxL7TbaJd0xduwcgyaJAx9Ld7toGL/gQ0od8batsjKC6lwB+Ia8OB/x9YOx7/QZq1d3gMJw1PpuRnjT6lr8W4dJSt3QAsyFq/vDvN2ZPEwnr1v+GtyUySMoCnDT54YWS6WcmVDpMgH1LXNShNqjVEQ8Sd8cSHGBayeSN0b0njyccg80ZjL2yMA9W3T0oRac5QvYOr36WVhQCMEw+Vd0SBL/4gBOdvSUnYM5r1roNqJqgzZjHHx9tpoSigp44v7Ac55P1KC7suXgCh9d2OSxumxoOPO9OiW4YyzwLVnkbsV4+K1C6KpoThjS/gBln3slCbUrh0RhFL3If4M/WNBnctKmlVz6GrLw9btNL0adfHgNZpJmDKyCuIH/WFqhiIggOHMmZiin1Lvd0BU0AA5a4+5BtkhsZRPdsTKfhEYdyepb0L1xnBPeZdS/1dAVNAAOeueQfw4nPEwlBj4zGQMDpeMwwQ53fr4oATwCWVfgkY1YNnCGL80BFRExI9Qb/y71REfDnsPQbMGzP8PKO1vTSLT/bxfFvgQHu8EubBoWYzvWwIqIuJHqDvu3TqID4et1wEiIuovJfeuNRhAXiCg0t3PibbAO+W5fgLkwcitGI+3RwIqIuqv0vCTXVAfAjYKILgNp2/w9VxRzWHFixhCLXGna4AdFMtw/RNEXKXYcwNRKlp4ciyTMuCRLJfDmc/jEGrpOwBEvAzPrCYCG8egGHZPDqK0jFW1+d9g+KYfb+UwRWH1HJeQYQn79sou9VI+nAo3lYfvW+LQ6u4QREIQCUEkiNIoVvWGFgxfDmHSU7TLhjMXuISoNYR8aw2Xehnemk2EUE9b4cGJIEJxm4K46pSyQpTnF5PzNnVzYdKFMWxVQj7Q0aVeShMLZAnly3rMuRDHx7NFNcP9m4HpvSqOWJCKqLcg1HoQHAbbcbGEqhYcidCj/JbbSaOWhT5PMQrDveVdhofOB6b3qjhiQQqi3oJQ734E37+2Qw27xqM4XDo+CPDLZFJQUkZUsaD81INZNzOEKGcsdCl53wLdZxXvO68VIurteqSKy6etAusawxfNcehwRwgF+NsZpKCY0eUtKN/1hq8k14FFy1xK7++BLnOK95nXFhH1dj1U0+XT1oTyy2q0mDwRRL0Fw/KzPCQQwssXE0VgKh4nZ8J7FbPLwMitLqWOBVouKtl9Vg9E1BuUmzu5YhoWvwxZQ68d13LDfNSwcCPGx/GiGvzQAwZX/zOPq87Er1DjAddAOzUWyqYTuFfS5APgrBuAV84oRoT9nVCAH7rB4Op/9uea5fgVat+PCMXLIICQagHDDpfhsi5oAKWerQwwrTGf1INSXwDvzE7BMHCLy2HnUWhb/f5DZL9MwIcqu+p6KDUsnh1fAqXvzhAqZoZQoXgIZUsGUqrbGgD5zfiwCWR+CWy6pyER+u1wOVx0D7Suede1tH6NgA/UcNX1UCpZSO854vvifF4Jv4alZ2MQStyDBEr7ozPAsJ68OBD4qhjMPNYSQ9NrUDDMOgkNGl/yBOU/w7dyY7arnpfDPRdBLm8PY+DjRAxnzvB1oohmiFiB4m2eg+tmU2J4Xl5eXv9hHXz0sdN9AAvuHtW0y8LyMHd1yU77wWH146hh+WkYIkQtULzts3DjNEoMz8vLy+s/rB3qYbjf4nbI+8oP3J5CYKHRI8tat5jZFepeVafKjg6kcNl1GKHaTQhieG0GVOaNUvR5HHoOycvLyxs4IAI8XBmH3rYTClFmvYrBgdtKIoazx6CB7LOYQPZ1TADhz8+IBADqPbK8dYvpPaDm1fUqb+tMhJJWUSrdiiCGE/OhEicr0OUZ6DEkLy8vb+BAB+GBWjh0s91QiDL1bSjGRavgo0zqj+yfl5fXf0Q1HC+lz2ZMAIRS961p3zR/EGQdaVFu41AUbCUUcx+Aw9FtUIGn2lHzTeg0rH9eXl7ekAwvh5p2CgYUvu+GUPkd2L4BIxxoiMSAt4to0OsjiLBxB3yWSamZ+fn5+ZNm5iLUeRAM3ewCf6QPnpSDu07vbrijJUFosh0D9P0QIpy3FT5Lp9Ts/Pz8/Emz+iHUfxARstaWRgD2HsBg2D3RVfI6QhToMnFQSYCUHn2yADIycB8R3H+VJUKVd+G6BTByen5+fv6UqakIj1RFYHY/BOCOtRhgXR1AOFwRCdS2JhKoVR0kADRrQHCFnImDSoHg9OhdCndV3Nc4uP+oRIQyH8DhZTBiRn5+fv6UaekID9VBYFYeAnDzhUTgtRZ0egVazp2Un58/aU4jHJafi8GwaiAaBIF2E4ZVBIHufSvjror78oq4/9uBCHwBG/dA/1mT8vPz82eUxnBzDgrjxiMAxkKE/BvhveoAt+L785+LYsojOavOxQjPd2bYPXTEp5B5o6ul3eDP4BYQAMHn/gg8lr36LIxwPJvhd9Een0LWjbj7Oa4P9fFWKMLaXMQwfSwaDCWmAggxI8wdBNOuqXwCDCuuhA+kE/5vSQGhekuUtI94uTwCzSGlMpS/BCGJKjEVQFzuCDOHwaSbyr4ChvlXwQeRDvi/sRgiVG2N4nzEi1VR0j+CGyeWroJPYepcBDhEmIp/cbmVIbOh21PyHhgGPwRvVWyu+L6sASJkdkENmzZ2vA9jeHwKbV9lCPQ4E+Pn1w+KYsLji3eDkPplhLkHWpbFMZ4KmTN+apwOFe3l/kCN4hZVPAVQus2HJxftBiHtS8O8K1uWxTGeCpmzf2yUBq3Gg5D62tTzAbTs81eWADYTshp1gSpuETy3w8qrV3VGlcfHUebFFq1xjDekNf5penFwFoNQ+ZVp5yLK2pN7rn+iEqxsFIZKCCohqIQBatQFqngKntthyfXndkeVB6dQ4qVWbXCMt5DW8Pu5xSCyVBDKvTptLSjjn4C7J+eA8RYocduDpUUYOQINAcSol6rgFjy3pjL6/iWjUcPhtXAiuxMmphCt+tG64sCiYmDYveOCughcXZ7il/SNwPmpxBawx4piQGcBkAZAh1L4rjdswKAsiNo7UX/hT2sJOQJgGgAdSuK7wbABg4pDOcWd2hm39B4ypBPMqouEE7bSMh+aVMfdEKjdGN/FBw4a1gDSSiNAVgfcPSaNnzIEcsZQOCrNpkGjWrgbArWa4LvYwEHDGkFqadzFO+IuVwGinfGfM2RID6g+n4QWqp0OtRrhrhuFSm3wnZI7aGhLiJTHs3VZ/I/pgvi7oYiWgPa5RKEJBbRYXYQEr58KiCvRm1J41ksDxFXwG2piIdQoQSJLDAFMI/wKxm4uoqmXABpAjFGPz0hg9RJAA4gx6kfUS41RCqgKbgFEAqgx4kPUQ40xSmGrglsAkQBqjPgQ9RABNIAaoxRQFQ8BRAKoMeJHhfCVSnZeEc1ThTi+aEFCEdUgQlBRP75FCCpxE9UgQlBRX76VoJK8RDWIEFTUl28ljoKnxE9VAgiBVfz4VnyKl/gy9LE5Rbl4GvZaCUUEQPz5dDyEkIWYkVjxFtziz1tAPYR4O7GStgiA+PMWiHgIiW9IVEMcU2KELPgUQnVYZEsX5Vqmhucw35ZFgwlUye0aIRLK1AwPSjcKhxrVvNKnJYZArbxscEKA0yIeVK4dktPSq0x+khOokZcDTghwmuNBhXohRVqHx3RNEMgeUAtSggmduiMebTQMgTaxOM0J5xJLEf6CeU+VQkJSmtohmEAKR7YO7zOz/taUQMKs1igILbbtPBcNpAxbc+NoFJQOC5D4CZm3nj+43/SGW8NYWhdA6Xn+wdloICHzsrOvRUHInYIkMSHtpguG9JveYFsYi+oDKF0vvGI+GkhIveys65FwhJqrEmTq0Vm9J3afNAENIFRaifeuuU9qCHD5onu9hCprw4A3fy6yKU0+NF+lhiZgd+MEEfhmHMB5loAi1DkPt+HNDpesCQF+KPHkcBdwcR0kXkLarz0ADn2NBFDan4kCwpc1b58VgsORJePvdgF7S5G8BeeXvgD7fwyktD4XBYRP6914eggOB84acT8aDspZ3dC4CeteBihuR2N8CXBxZQSU3OezPid4hFVXtj3hhbK8LxqCvbnIJmy+hfh+/wqBlYcOo2rgBRBAYwDndUJxW8JU6vxOTKXTeky84LX1OOpQ/BmC76mDgFLCEvJPjcFDGboEk7TgxGYi6pD2LIg/2NkIAUi1hPxNK8IXql6CxEupacFRhy2ziAISA6h+KYL7ui2E+8ZoYgsVLkWCCBl2QpEthZfWI3EwXG4DCY7NQgGWZUCxMmBKSnFIKQF34ymDfommhgBnv0EUdQF3Em+lmgUBWAFklQInSzMhrRRyB96zPiM1jJRatqYDIECxW0naQjkLArBCIasUOFlaDFJKwV14539Fahip1Wy9SCjqAm6PEG/D/UeJgtJuOFDWgdJkQWqJdJZNxQCpfDIrlLRif3fxARxNI6hhiC1XRBPKLrYXjieOSjvbHPWndLb4XDC0+4bS7LA94M73pfRdXl1ffndKIySQDPrukckVIOp1fVUkPoaJFp9L+/fd4HCNbQHPHqfJEdTV75MX8msgQYSWt/05owslsovhPpqRtAzDLD6X5vXakMoh2xaefoV6N6KuPh+cmFILCSK0uMnO7IqEABHAsKszGifhz+UYvNss6TBvHOfZ/nDTJxU53A5BqLrKnjsymNDgUjs/F4lh2NYTDXSTpagu8AegIqLhAPZijD9DrgXxOn8Z1Hoc3qwIg7LouxsDKHecDqiIqC94vy3Um/eoy3C4Lxqv2X6umAQd7oUPFUYpk9ZiXBwfCKIiIn4wLL8RKozscdt4xHBXcyRp5fu5dBq0egDeT4GRUcaeh3HxzEgQFRHxg2HBHeCSAJGut8xDcdgwFxMnsPNilXoGuKEXr9WC/qXgrhIASukfABUR9YMh9yVARUQ9zlmMCQB2K6aIZpj4FQ7xNNz8OwGVhhZvh98yiHJyLCtvIWs0jL/IA2w2hsBCxGZByfqPe10+NH65sSTjd1A+y2HXbioPgSXLY9iaCMGV11bAFIj87RDhto5o0uoWS1N/hwgf9+Liy6gwHOafHcPWQwiunFhNCiE6kfMWYTCcc0bclK824bhS2HGQTHp+wtx7yRgHPCkuw9knSSG44ehhUohtWHZuEKWGrYUW2S69DpiSU3b4rGg4SjNbDfUF2C5EQRhYzkKE51fAt3RoFEuIWqDZvPRu0zsjiHoorX8DgSe9Dg2Pl4CtiePqV9+C4eNxlP6avlVhqYdQ1QKdZqT1ntUSQdQL/mwMr7aGr1oS5a54qISgEoJKWICtj+PqX9uC4f0pZH1NrxqxhPIWaD8zrdes1giiXvBba6AJqa1T/AlnLvZYszJuhtUfYkBpm3XXxTi0+hO+pXlzX3ddBDK7VeXR010qHsJHI6D4wpoNJ0zwWLk6iGGvpciuvDsdBtb6bRBXr8CEAfxyGOPPMONbQKgyBVsO5fNucGRJO6DHPhQMud9B3ab7HqPSp/g0rD6GgfSnvA73jheGtW8BQuNhWIBf6sGDEzuijN3oMsx4G1rUu/UmWr2BElMoY4FUkO/B4c6mSGgF3nDWB4DQYBQ2AvzYGO6Z1hFl+GaXIf9DaFL/hqM0fRslplDcAp3WXd2x1RP+DKtcDhtmxQ34qz8oLGLtXaQx+WU4cHYbBe4qjQD80A8GNfkrm4fGoXgL2ApE+vX7tQrvNnKdsygI2DlEimgGbGUo3eI4HJkTkmGkJfiFD5eDikuKMeNKGHAUoapthKH0nQApbLkdKvNYNlXfxnDB9R4pPHaOK/VJF9xQAYkTcODWYlBnYYRVW2DKIQztbXkMDa5HIYUbdkE1TtSi/Qs4XLETBaIM+QQBuHQGBo5GCf/ujWigWy5GAwjX7kVDAi69vQTUWBxl5Q6YcA2GFrYyhto3opDCVZdDVY43pMVLOFy6DwWi5H4BFWtPeQy+LB3gHJeyry0SL6XSB2OA0bnwTE14uAWUs21Qw/7OKAZsBC3W5yF4ZCCU/6gaAg71/4R0WbIX3q6KKDs7o74MwyxF+IZ/QYR1O+HT4oRul+MEocOODcsnRIGBsyfMxH0+7ttc8MZ0HPgC1u7DYeMv4oI/WqFC2tNed5KQvXesXzIaYMz0yfm4L8B9G55f98Uh62O4dDURrvwY76sOYVBGLwIoeQtxfOziQMK9e0K49WA86Llj/bIxAoycMWkK7gtw34bn54NxSP8Udm/AYe/neF9+BIUXsqnyHoy8ZvfevXv3XJVNlFVLMMCdJKDAqovOndsaKLZ05KKGABujYJg1GwP0+QaUK1bAF0AZOxcFOOtpDBzrS933cN8eJehP52OKZkLXyOQ7MMKzXRl0P+2RUAz9LYGFgIK3YWUvcAbzTSqGAY/A61UbO9C+GEKFbP0Gd8qjgNL7HEwieAtBDRc0haxefAcRZt4A72e0BroZ1xhe7oxCwwGQChPmYMJL4oZNbSCzL9+BIf9WeC+rFdDFcY3hhZ4YeC/K6l1k4dNw5gJXw11o/FBiCv6FUvtRclm9FwfeqMesw7QEuiIwkqOnuT4W9ixPqQ219iC+DLmWorpi8w+1QnG+SWHBgeYVCf2djZgAqIK4FAS3AkL5rbDg565HEMPhDfBauy5QYg443PD4pA0YwLyCe3M5JBFUQAAUxEMBocF62P7G4D0Yw31zyHq1WUeoNA2U5rbZwyDUOz2j+OTqsCOFOKqEoBqCalxUQFwK4qGAUPs8OO/9vP0Yw52LyDjZvBNUOA2ERrbZw6D0fhveatEzww+sPwuEc5ojCQARQAAREJcCCOsage1/WxWUSv+FTZvblIDxDVCK2VaPgKH3a3DL7M4CK9sEgL/HYYpoQueFfQAirYBuZQhbKGGz0AChCxP6UWzqXAFonA7VOwKd6iBClVmnAUKz8184tw70Ho9QoOd2oPyMmbibG6jfCuhREQHGLiyHCFe8/+K7j8HoAUgckrkwszNlps/C3cyBem2A7pUQYMziiogy5DQYt7AcvsfceXS00HIeBTxzLfRalI1AWlOgZzFIG4h76OL6KJStDdFeCo0X4t9w9quc8hqmvkrijipOYhYfRYEfGyUxyw2j8B6bSkHVsRRwoUk3EnYs/oWKv4MU3QTxUEDjANw9Gk0UBERcAoj6ECSGAkJyFA8BRPwI4hJVVYTCXEA8BBDxI4hLBCSQCMlSEA8F1CUegrhEAMUtAR7tRwQ3VlA4IPYAAABwGwCdASrQApkAPm02mUmkIyKhICgAgA2JaW7hd2EbQAnsA99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+2TkPfbJyHvtk5D32ych77ZOQ99snIe+sAAA/v/rAAAAAAAAAAAAAAAAAABFWElGugAAAEV4aWYAAElJKgAIAAAABgASAQMAAQAAAAEAAAAaAQUAAQAAAFYAAAAbAQUAAQAAAF4AAAAoAQMAAQAAAAIAAAATAgMAAQAAAAEAAABphwQAAQAAAGYAAAAAAAAASAAAAAEAAABIAAAAAQAAAAYAAJAHAAQAAAAwMjEwAZEHAAQAAAABAgMAAKAHAAQAAAAwMTAwAaADAAEAAAD//wAAAqAEAAEAAADQAgAAA6AEAAEAAACZAAAAAAAAAA==)\n",
        "\n",
        "we use the pairwise covariance of the different features to determine how they relate to each other. With these covariances, our goal is to group / cluster based on similar patterns. Intuitively, we can relate features if they have similar covariances with other features."
      ],
      "metadata": {
        "id": "7rzoiQ7fMk_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Calculate the Covariance Matrix\n",
        "\n",
        "---\n",
        "with using numpy with arguments (ddof =0, rowvar = False)\n"
      ],
      "metadata": {
        "id": "uuhux3UEcBgw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn8oujZlK9YR",
        "outputId": "c33e89dd-8420-40bb-aabf-57927e8b6086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance Matrix:\n",
            "[[ 1.         -0.35082321 -0.9029865   0.75182742 -0.30833773]\n",
            " [-0.35082321  1.          0.16997503 -0.64291136  0.98959863]\n",
            " [-0.9029865   0.16997503  1.         -0.49956057  0.18506849]\n",
            " [ 0.75182742 -0.64291136 -0.49956057  1.         -0.58348328]\n",
            " [-0.30833773  0.98959863  0.18506849 -0.58348328  1.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate the covariance matrix\n",
        "cov_matrix = np.cov(standardized_data, ddof=0, rowvar=False)\n",
        "\n",
        "# Print the covariance matrix\n",
        "print(\"Covariance Matrix:\")\n",
        "print(cov_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Eigendecomposition on the Covariance Matrix\n",
        "\n",
        "Hint:  [Using numpy](https://https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html)"
      ],
      "metadata": {
        "id": "uXNcG4AFcT08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Perform eigendecomposition\n",
        "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# Print the eigenvalues and eigenvectors\n",
        "print(\"Eigenvalues:\")\n",
        "print(eigenvalues)\n",
        "\n",
        "print(\"\\nEigenvectors:\")\n",
        "print(eigenvectors)\n"
      ],
      "metadata": {
        "id": "dmGlQ47tRO5w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe972c4-ec45-464e-def1-d0895d75d1b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eigenvalues:\n",
            "[3.17488134e+00 1.44713013e+00 4.12109190e-02 3.95157890e-05\n",
            " 3.36738100e-01]\n",
            "\n",
            "Eigenvectors:\n",
            "[[-0.4640131   0.45182808 -0.70733581  0.28128049 -0.03317471]\n",
            " [ 0.45019005  0.48800851  0.29051532  0.6706731  -0.15803498]\n",
            " [ 0.37929082 -0.55665017 -0.48462321  0.24186072 -0.5029143 ]\n",
            " [-0.4976889   0.03162214  0.36999674 -0.03373724 -0.78311558]\n",
            " [ 0.43642295  0.49682965 -0.20861365 -0.64143906 -0.32822489]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Sort the Principal Components\n",
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list"
      ],
      "metadata": {
        "id": "4pWho88fcbJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort can only provide lowest to highest; use [::-1] to reverse the list\n",
        "\n",
        "order_of_importance = np.argsort(eigenvalues)[::-1]\n",
        "print ( 'the order of importance is :\\n {}'.format(order_of_importance))\n",
        "\n",
        "# utilize the sort order to sort eigenvalues and eigenvectors\n",
        "sorted_eigenvalues = eigenvalues[order_of_importance]\n",
        "\n",
        "print('\\n\\n sorted eigen values:\\n{}'.format(sorted_eigenvalues))\n",
        "sorted_eigenvectors = eigenvectors[:, order_of_importance] # sort the columns\n",
        "print('\\n\\n The sorted eigen vector matrix is: \\n {}'.format(sorted_eigenvectors))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_znKtzdrTmMg",
        "outputId": "101549ee-bcbb-4d1f-bda6-aa2f595e8425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the order of importance is :\n",
            " [0 1 4 2 3]\n",
            "\n",
            "\n",
            " sorted eigen values:\n",
            "[3.17488134e+00 1.44713013e+00 3.36738100e-01 4.12109190e-02\n",
            " 3.95157890e-05]\n",
            "\n",
            "\n",
            " The sorted eigen vector matrix is: \n",
            " [[-0.4640131   0.45182808 -0.03317471 -0.70733581  0.28128049]\n",
            " [ 0.45019005  0.48800851 -0.15803498  0.29051532  0.6706731 ]\n",
            " [ 0.37929082 -0.55665017 -0.5029143  -0.48462321  0.24186072]\n",
            " [-0.4976889   0.03162214 -0.78311558  0.36999674 -0.03373724]\n",
            " [ 0.43642295  0.49682965 -0.32822489 -0.20861365 -0.64143906]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question:\n",
        "\n",
        "1. Why do we order eigen values and eigen vectors?\n",
        "\n",
        "The ordering of eigenvalues and corresponding eigenvectors is crucial in Principal Component Analysis (PCA) for the following reasons:\n",
        "\n",
        "\n",
        "\n",
        "*   Identifying Principal Components:\n",
        "\n",
        "The eigenvalues represent the variance captured by each principal component. Sorting them in descending order helps identify which principal components contribute the most to the total variance in the data.\n",
        "\n",
        "*   Reducing Dimensionality:\n",
        "\n",
        "\n",
        "Principal components are ordered by the amount of variance they explain. By selecting the top\n",
        "k principal components (those corresponding to the largest eigenvalues), we can achieve dimensionality reduction while retaining the most important information.\n",
        "\n",
        "\n",
        "*   Interpretability:\n",
        "\n",
        "\n",
        "\n",
        "Ordering eigenvalues allows us to prioritize principal components in terms of their significance. This makes it easier to interpret the results and understand the relative importance of each component.\n",
        "\n",
        "\n",
        "*   Eigenvalue Thresholding:\n",
        "\n",
        "In some cases, eigenvalues may be very small, indicating that the corresponding principal components contribute little to the overall variance. Ordering helps in identifying and potentially discarding components with negligible contributions, aiding in model simplification.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Is it true we would consider the lowest eigen value compared to the highest? Defend your answer\n",
        "\n",
        "\n",
        "It is not true that we would consider the lowest eigenvalue compared to the highest when performing PCA. The opposite is true; we prioritize the highest eigenvalues. Here's the reasoning:\n",
        "\n",
        "*   Variance Capture:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Principal components corresponding to higher eigenvalues capture more variance in the data. Considering the highest eigenvalues ensures that we retain the most significant information about the dataset.\n",
        "* Dimensionality Reduction:\n",
        "\n",
        "The goal of PCA is often to reduce dimensionality while preserving as much variance as possible. This is achieved by selecting the top principal components, which correspond to the highest eigenvalues.\n",
        "* Information Retention:\n",
        "\n",
        "The eigenvalues represent the amount of information retained in each principal component. Higher eigenvalues indicate that the corresponding principal components contain more information about the original features.\n",
        "In summary, the highest eigenvalues are considered in PCA because they represent the principal components that capture the most information and variance in the dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o1nILNGxpTJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You want to see what percentage of information each eigen value holds. You would have print out the percentage of each eigen value using the formula\n",
        "\n",
        "\n",
        "\n",
        "> (sorted eigen values / sum of all sorted eigen values) * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "BWqFGNeNvgEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use sorted_eigenvalues to ensure the explained variances correspond to the eigenvectors\n",
        "# Given eigenvalues\n",
        "sorted_eigenvalues = np.array([3.17488134e+00, 1.44713013e+00, 3.36738100e-01, 4.12109190e-02, 3.95157890e-05])\n",
        "\n",
        "\n",
        "explained_variance = (sorted_eigenvalues / np.sum(sorted_eigenvalues)) * 100\n",
        "explained_variance =[\"{:.2f}%\".format(value) for value in explained_variance]\n",
        "print( explained_variance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRMHrffrVOXR",
        "outputId": "5c72fc34-75be-4d53-fd6b-16f9676e365c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['63.50%', '28.94%', '6.73%', '0.82%', '0.00%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize the number of Principle components then perfrom matrix multiplication with the variable K example k = 3 for 3 priciple components\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> The reulting matrix (with reduced data) = standardized data * vector with columns k\n",
        "\n",
        "See expected output for k = 2\n",
        "\n"
      ],
      "metadata": {
        "id": "qB7H4InbfKx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Given standardized data\n",
        "standardized_data = np.array([\n",
        "    [-1.36438208,  0.70710678,  1.5109662,  -0.99186978,  0.77802924],\n",
        "    [ 0.12403473, -1.94454365, -0.13736056,  0.77145428, -2.06841919],\n",
        "    [-0.62017367,  0.1767767,   0.68680282, -0.99186978,  0.20873955],\n",
        "    [ 1.61245155,  0.1767767,  -1.78568733,  0.33062326,  0.20873955],\n",
        "    [-0.62017367,  1.23743687, -0.13736056, -0.77145428,  1.00574511],\n",
        "    [ 0.86824314, -0.35355339, -0.13736056,  1.65311631, -0.13283426],\n",
        "])\n",
        "\n",
        "# Specify the number of principal components (k)\n",
        "k = 3\n",
        "\n",
        "# Given sorted eigenvectors (considering k)\n",
        "selected_eigenvectors = sorted_eigenvectors[:, :k]\n",
        "\n",
        "# Perform matrix multiplication to obtain reduced data\n",
        "reduced_data = np.matmul(standardized_data, selected_eigenvectors)\n",
        "\n",
        "# Print the resulting matrix with reduced data\n",
        "print(\"Resulting Matrix (with reduced data) for k={}:\".format(k))\n",
        "# Print the shape of the reduced_data matrix\n",
        "print(\"Shape of the reduced_data matrix:\", reduced_data.shape)\n"
      ],
      "metadata": {
        "id": "C-Rnyq6QVTiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64e1b47-90ce-450b-fd81-a3c8190ff595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resulting Matrix (with reduced data) for k=3:\n",
            "Shape of the reduced_data matrix: (6, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxxBcgQMXe1h",
        "outputId": "1f3b3189-ae7a-4fbe-d217-52559a156e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 2.3577116  -0.75728867 -0.30499103]\n",
            " [-2.27171739 -1.81970664  0.4470405 ]\n",
            " [ 1.21259113 -0.50390931  0.35546937]\n",
            " [-1.41935913  1.9229856   0.48918845]\n",
            " [ 1.61562536  0.87541858  0.16812364]\n",
            " [-1.49485157  0.28250043 -1.15483094]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reduced_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEqS6cuaMSY",
        "outputId": "0b1b383f-eddf-4907-b978-82f0e1939353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *What are 2 positive effects and 2 negative effects of PCA\n",
        "\n",
        "Give 2 Benefits and 2 limitations\n",
        "\n",
        "### Positive Effects of PCA:\n",
        "\n",
        "1. **Dimensionality Reduction:**\n",
        "   - **Positive Effect:** PCA helps in reducing the dimensionality of the dataset by transforming it into a new set of uncorrelated variables called principal components. This simplification can lead to improved computational efficiency, less storage space, and faster learning algorithms.\n",
        "\n",
        "2. **Noise Reduction and Feature Selection:**\n",
        "   - **Positive Effect:** PCA tends to emphasize the directions in which the data varies the most, capturing the most significant information. By focusing on the principal components with higher eigenvalues, PCA can reduce the impact of noise and highlight the essential features, promoting more robust and accurate models.\n",
        "\n",
        "### Negative Effects of PCA:\n",
        "\n",
        "1. **Loss of Interpretability:**\n",
        "   - **Negative Effect:** The principal components obtained from PCA are linear combinations of the original features, making them less interpretable in terms of the original variables. While PCA is excellent for reducing dimensionality, it may result in a loss of direct insight into the meaning of the transformed features.\n",
        "\n",
        "2. **Assumption of Linearity:**\n",
        "   - **Negative Effect:** PCA assumes that the relationships between variables are linear. If the underlying relationships in the data are nonlinear, PCA may not capture the essential structure, leading to suboptimal results. Nonlinear variants of dimensionality reduction techniques may be more appropriate in such cases.\n",
        "\n",
        "It's important to note that the impact of PCA depends on the specific characteristics of the data and the goals of the analysis. While PCA can offer significant advantages, it's essential to consider its limitations and potential drawbacks in certain scenarios."
      ],
      "metadata": {
        "id": "UxQ8lTunauMQ"
      }
    }
  ]
}